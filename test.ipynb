{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=1e10, solver='lbfgs') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_new = LogisticRegression(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_probs = [0.61051559, 0.00047493709, 0.99639291, 0.00021221573, 0.99599433, 0.0014127002, 0.0028262993]\n",
    "labels = [1,0,1,0,1,0,0]\n",
    "\n",
    "import numpy as np\n",
    "# turn into two-column array, with the i-th column be the probability of the i-th class\n",
    "raw_probs = np.array(raw_probs) \n",
    "raw_probs = np.vstack((raw_probs, 1-raw_probs)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(raw_probs, labels)\n",
    "clf.predict(raw_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "EPOCHS = 1000\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.SGD(clf_new.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.1052e-01, 3.8948e-01],\n",
      "        [4.7494e-04, 9.9953e-01],\n",
      "        [9.9639e-01, 3.6071e-03],\n",
      "        [2.1222e-04, 9.9979e-01],\n",
      "        [9.9599e-01, 4.0057e-03],\n",
      "        [1.4127e-03, 9.9859e-01],\n",
      "        [2.8263e-03, 9.9717e-01]])\n",
      "torch.Size([7, 2])\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "torch.Size([7, 1])\n"
     ]
    }
   ],
   "source": [
    "raw_probs_tensor = [0.61051559, 0.00047493709, 0.99639291, 0.00021221573, 0.99599433, 0.0014127002, 0.0028262993]\n",
    "\n",
    "raw_probs_tensor = torch.tensor([[raw_prob, 1-raw_prob] for raw_prob in raw_probs_tensor])\n",
    "label_tensor = torch.tensor([[float(label)] for label in labels])\n",
    "print(raw_probs_tensor)\n",
    "print(raw_probs_tensor.size())\n",
    "print(label_tensor)\n",
    "print(label_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t| Train loss: 0.269 \t| Test loss: 0.269 \t|\n",
      "Epoch: 200 \t| Train loss: 0.168 \t| Test loss: 0.168 \t|\n",
      "Epoch: 400 \t| Train loss: 0.124 \t| Test loss: 0.124 \t|\n",
      "Epoch: 600 \t| Train loss: 0.099 \t| Test loss: 0.099 \t|\n",
      "Epoch: 800 \t| Train loss: 0.083 \t| Test loss: 0.083 \t|\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses  = []\n",
    "train_accs = []\n",
    "test_accs  = []\n",
    "\n",
    "# print(torch.t(torch.tensor([labels])))\n",
    "for epoch in range(EPOCHS):\n",
    "    # Forward propagation (predicting train data) #a\n",
    "    train_preds = clf_new(raw_probs_tensor)\n",
    "    train_loss  = loss_function(train_preds, label_tensor)\n",
    "    # print(train_loss)\n",
    "    \n",
    "    # Predicting test data #b\n",
    "    with torch.no_grad():\n",
    "        test_preds = clf_new(raw_probs_tensor)\n",
    "        test_loss  = loss_function(test_preds, label_tensor)\n",
    "        \n",
    "    # Backward propagation #d\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "\n",
    "    # Gradient descent step #e\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Store training history #f\n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    if epoch%200==0:\n",
    "        print(f'Epoch: {epoch} \\t|' \\\n",
    "            f' Train loss: {np.round(train_loss.item(),3)} \\t|' \\\n",
    "            f' Test loss: {np.round(test_loss.item(),3)} \\t|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7855],\n",
       "        [0.0538],\n",
       "        [0.9808],\n",
       "        [0.0538],\n",
       "        [0.9807],\n",
       "        [0.0542],\n",
       "        [0.0547]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_new(raw_probs_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
