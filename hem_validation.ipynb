{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "import calibration as cal\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained('vectara/hallucination_evaluation_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('vectara/hallucination_evaluation_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    [\"A man walks into a bar and buys a drink\", \"A bloke swigs alcohol at a pub\"],\n",
    "    [\"A person on a horse jumps over a broken down airplane.\", \"A person is at a diner, ordering an omelette.\"],\n",
    "    [\"A person on a horse jumps over a broken down airplane.\", \"A person is outdoors, on a horse.\"],\n",
    "    [\"A boy is jumping on skateboard in the middle of a red bridge.\", \"The boy skates down the sidewalk on a blue bridge\"],\n",
    "    [\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\", \"A blond drinking water in public.\"],\n",
    "    [\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\", \"A blond man wearing a brown shirt is reading a book.\"],\n",
    "    [\"Mark Wahlberg was a fan of Manny.\", \"Manny was a fan of Mark Wahlberg.\"], \n",
    "]\n",
    "\n",
    "inputs = tokenizer.batch_encode_plus(pairs, return_tensors='pt', padding=True)\n",
    "print(inputs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4494805]\n",
      " [-7.651853 ]\n",
      " [ 5.6212516]\n",
      " [-8.457695 ]\n",
      " [ 5.5160384]\n",
      " [-6.56084  ]\n",
      " [-5.86595  ]]\n",
      "[6.1051571e-01 4.7493709e-04 9.9639291e-01 2.1221573e-04 9.9599433e-01\n",
      " 1.4126982e-03 2.8263179e-03]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits.cpu().detach().numpy()\n",
    "    print(logits)\n",
    "    # convert logits to probabilities\n",
    "    scores = 1 / (1 + np.exp(-logits)).flatten()\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4495],\n",
      "        [-7.6519],\n",
      "        [ 5.6213],\n",
      "        [-8.4577],\n",
      "        [ 5.5160],\n",
      "        [-6.5608],\n",
      "        [-5.8660]]), hidden_states=None, attentions=None)\n",
      "tensor([[ 0.4495],\n",
      "        [-7.6519],\n",
      "        [ 5.6213],\n",
      "        [-8.4577],\n",
      "        [ 5.5160],\n",
      "        [-6.5608],\n",
      "        [-5.8660]])\n",
      "tensor([[6.1052e-01],\n",
      "        [4.7494e-04],\n",
      "        [9.9639e-01],\n",
      "        [2.1222e-04],\n",
      "        [9.9599e-01],\n",
      "        [1.4127e-03],\n",
      "        [2.8263e-03]])\n"
     ]
    }
   ],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs)\n",
    "    logits = outputs.logits\n",
    "    print(logits)\n",
    "    # convert logits to probabilities\n",
    "    scores = sigmoid(logits)\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "deberta\n",
      "deberta.embeddings\n",
      "deberta.embeddings.word_embeddings\n",
      "deberta.embeddings.LayerNorm\n",
      "deberta.embeddings.dropout\n",
      "deberta.encoder\n",
      "deberta.encoder.layer\n",
      "deberta.encoder.layer.0\n",
      "deberta.encoder.layer.0.attention\n",
      "deberta.encoder.layer.0.attention.self\n",
      "deberta.encoder.layer.0.attention.self.query_proj\n",
      "deberta.encoder.layer.0.attention.self.key_proj\n",
      "deberta.encoder.layer.0.attention.self.value_proj\n",
      "deberta.encoder.layer.0.attention.self.pos_dropout\n",
      "deberta.encoder.layer.0.attention.self.dropout\n",
      "deberta.encoder.layer.0.attention.output\n",
      "deberta.encoder.layer.0.attention.output.dense\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm\n",
      "deberta.encoder.layer.0.attention.output.dropout\n",
      "deberta.encoder.layer.0.intermediate\n",
      "deberta.encoder.layer.0.intermediate.dense\n",
      "deberta.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "deberta.encoder.layer.0.output\n",
      "deberta.encoder.layer.0.output.dense\n",
      "deberta.encoder.layer.0.output.LayerNorm\n",
      "deberta.encoder.layer.0.output.dropout\n",
      "deberta.encoder.layer.1\n",
      "deberta.encoder.layer.1.attention\n",
      "deberta.encoder.layer.1.attention.self\n",
      "deberta.encoder.layer.1.attention.self.query_proj\n",
      "deberta.encoder.layer.1.attention.self.key_proj\n",
      "deberta.encoder.layer.1.attention.self.value_proj\n",
      "deberta.encoder.layer.1.attention.self.pos_dropout\n",
      "deberta.encoder.layer.1.attention.self.dropout\n",
      "deberta.encoder.layer.1.attention.output\n",
      "deberta.encoder.layer.1.attention.output.dense\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm\n",
      "deberta.encoder.layer.1.attention.output.dropout\n",
      "deberta.encoder.layer.1.intermediate\n",
      "deberta.encoder.layer.1.intermediate.dense\n",
      "deberta.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "deberta.encoder.layer.1.output\n",
      "deberta.encoder.layer.1.output.dense\n",
      "deberta.encoder.layer.1.output.LayerNorm\n",
      "deberta.encoder.layer.1.output.dropout\n",
      "deberta.encoder.layer.2\n",
      "deberta.encoder.layer.2.attention\n",
      "deberta.encoder.layer.2.attention.self\n",
      "deberta.encoder.layer.2.attention.self.query_proj\n",
      "deberta.encoder.layer.2.attention.self.key_proj\n",
      "deberta.encoder.layer.2.attention.self.value_proj\n",
      "deberta.encoder.layer.2.attention.self.pos_dropout\n",
      "deberta.encoder.layer.2.attention.self.dropout\n",
      "deberta.encoder.layer.2.attention.output\n",
      "deberta.encoder.layer.2.attention.output.dense\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm\n",
      "deberta.encoder.layer.2.attention.output.dropout\n",
      "deberta.encoder.layer.2.intermediate\n",
      "deberta.encoder.layer.2.intermediate.dense\n",
      "deberta.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "deberta.encoder.layer.2.output\n",
      "deberta.encoder.layer.2.output.dense\n",
      "deberta.encoder.layer.2.output.LayerNorm\n",
      "deberta.encoder.layer.2.output.dropout\n",
      "deberta.encoder.layer.3\n",
      "deberta.encoder.layer.3.attention\n",
      "deberta.encoder.layer.3.attention.self\n",
      "deberta.encoder.layer.3.attention.self.query_proj\n",
      "deberta.encoder.layer.3.attention.self.key_proj\n",
      "deberta.encoder.layer.3.attention.self.value_proj\n",
      "deberta.encoder.layer.3.attention.self.pos_dropout\n",
      "deberta.encoder.layer.3.attention.self.dropout\n",
      "deberta.encoder.layer.3.attention.output\n",
      "deberta.encoder.layer.3.attention.output.dense\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm\n",
      "deberta.encoder.layer.3.attention.output.dropout\n",
      "deberta.encoder.layer.3.intermediate\n",
      "deberta.encoder.layer.3.intermediate.dense\n",
      "deberta.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "deberta.encoder.layer.3.output\n",
      "deberta.encoder.layer.3.output.dense\n",
      "deberta.encoder.layer.3.output.LayerNorm\n",
      "deberta.encoder.layer.3.output.dropout\n",
      "deberta.encoder.layer.4\n",
      "deberta.encoder.layer.4.attention\n",
      "deberta.encoder.layer.4.attention.self\n",
      "deberta.encoder.layer.4.attention.self.query_proj\n",
      "deberta.encoder.layer.4.attention.self.key_proj\n",
      "deberta.encoder.layer.4.attention.self.value_proj\n",
      "deberta.encoder.layer.4.attention.self.pos_dropout\n",
      "deberta.encoder.layer.4.attention.self.dropout\n",
      "deberta.encoder.layer.4.attention.output\n",
      "deberta.encoder.layer.4.attention.output.dense\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm\n",
      "deberta.encoder.layer.4.attention.output.dropout\n",
      "deberta.encoder.layer.4.intermediate\n",
      "deberta.encoder.layer.4.intermediate.dense\n",
      "deberta.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "deberta.encoder.layer.4.output\n",
      "deberta.encoder.layer.4.output.dense\n",
      "deberta.encoder.layer.4.output.LayerNorm\n",
      "deberta.encoder.layer.4.output.dropout\n",
      "deberta.encoder.layer.5\n",
      "deberta.encoder.layer.5.attention\n",
      "deberta.encoder.layer.5.attention.self\n",
      "deberta.encoder.layer.5.attention.self.query_proj\n",
      "deberta.encoder.layer.5.attention.self.key_proj\n",
      "deberta.encoder.layer.5.attention.self.value_proj\n",
      "deberta.encoder.layer.5.attention.self.pos_dropout\n",
      "deberta.encoder.layer.5.attention.self.dropout\n",
      "deberta.encoder.layer.5.attention.output\n",
      "deberta.encoder.layer.5.attention.output.dense\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm\n",
      "deberta.encoder.layer.5.attention.output.dropout\n",
      "deberta.encoder.layer.5.intermediate\n",
      "deberta.encoder.layer.5.intermediate.dense\n",
      "deberta.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "deberta.encoder.layer.5.output\n",
      "deberta.encoder.layer.5.output.dense\n",
      "deberta.encoder.layer.5.output.LayerNorm\n",
      "deberta.encoder.layer.5.output.dropout\n",
      "deberta.encoder.layer.6\n",
      "deberta.encoder.layer.6.attention\n",
      "deberta.encoder.layer.6.attention.self\n",
      "deberta.encoder.layer.6.attention.self.query_proj\n",
      "deberta.encoder.layer.6.attention.self.key_proj\n",
      "deberta.encoder.layer.6.attention.self.value_proj\n",
      "deberta.encoder.layer.6.attention.self.pos_dropout\n",
      "deberta.encoder.layer.6.attention.self.dropout\n",
      "deberta.encoder.layer.6.attention.output\n",
      "deberta.encoder.layer.6.attention.output.dense\n",
      "deberta.encoder.layer.6.attention.output.LayerNorm\n",
      "deberta.encoder.layer.6.attention.output.dropout\n",
      "deberta.encoder.layer.6.intermediate\n",
      "deberta.encoder.layer.6.intermediate.dense\n",
      "deberta.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "deberta.encoder.layer.6.output\n",
      "deberta.encoder.layer.6.output.dense\n",
      "deberta.encoder.layer.6.output.LayerNorm\n",
      "deberta.encoder.layer.6.output.dropout\n",
      "deberta.encoder.layer.7\n",
      "deberta.encoder.layer.7.attention\n",
      "deberta.encoder.layer.7.attention.self\n",
      "deberta.encoder.layer.7.attention.self.query_proj\n",
      "deberta.encoder.layer.7.attention.self.key_proj\n",
      "deberta.encoder.layer.7.attention.self.value_proj\n",
      "deberta.encoder.layer.7.attention.self.pos_dropout\n",
      "deberta.encoder.layer.7.attention.self.dropout\n",
      "deberta.encoder.layer.7.attention.output\n",
      "deberta.encoder.layer.7.attention.output.dense\n",
      "deberta.encoder.layer.7.attention.output.LayerNorm\n",
      "deberta.encoder.layer.7.attention.output.dropout\n",
      "deberta.encoder.layer.7.intermediate\n",
      "deberta.encoder.layer.7.intermediate.dense\n",
      "deberta.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "deberta.encoder.layer.7.output\n",
      "deberta.encoder.layer.7.output.dense\n",
      "deberta.encoder.layer.7.output.LayerNorm\n",
      "deberta.encoder.layer.7.output.dropout\n",
      "deberta.encoder.layer.8\n",
      "deberta.encoder.layer.8.attention\n",
      "deberta.encoder.layer.8.attention.self\n",
      "deberta.encoder.layer.8.attention.self.query_proj\n",
      "deberta.encoder.layer.8.attention.self.key_proj\n",
      "deberta.encoder.layer.8.attention.self.value_proj\n",
      "deberta.encoder.layer.8.attention.self.pos_dropout\n",
      "deberta.encoder.layer.8.attention.self.dropout\n",
      "deberta.encoder.layer.8.attention.output\n",
      "deberta.encoder.layer.8.attention.output.dense\n",
      "deberta.encoder.layer.8.attention.output.LayerNorm\n",
      "deberta.encoder.layer.8.attention.output.dropout\n",
      "deberta.encoder.layer.8.intermediate\n",
      "deberta.encoder.layer.8.intermediate.dense\n",
      "deberta.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "deberta.encoder.layer.8.output\n",
      "deberta.encoder.layer.8.output.dense\n",
      "deberta.encoder.layer.8.output.LayerNorm\n",
      "deberta.encoder.layer.8.output.dropout\n",
      "deberta.encoder.layer.9\n",
      "deberta.encoder.layer.9.attention\n",
      "deberta.encoder.layer.9.attention.self\n",
      "deberta.encoder.layer.9.attention.self.query_proj\n",
      "deberta.encoder.layer.9.attention.self.key_proj\n",
      "deberta.encoder.layer.9.attention.self.value_proj\n",
      "deberta.encoder.layer.9.attention.self.pos_dropout\n",
      "deberta.encoder.layer.9.attention.self.dropout\n",
      "deberta.encoder.layer.9.attention.output\n",
      "deberta.encoder.layer.9.attention.output.dense\n",
      "deberta.encoder.layer.9.attention.output.LayerNorm\n",
      "deberta.encoder.layer.9.attention.output.dropout\n",
      "deberta.encoder.layer.9.intermediate\n",
      "deberta.encoder.layer.9.intermediate.dense\n",
      "deberta.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "deberta.encoder.layer.9.output\n",
      "deberta.encoder.layer.9.output.dense\n",
      "deberta.encoder.layer.9.output.LayerNorm\n",
      "deberta.encoder.layer.9.output.dropout\n",
      "deberta.encoder.layer.10\n",
      "deberta.encoder.layer.10.attention\n",
      "deberta.encoder.layer.10.attention.self\n",
      "deberta.encoder.layer.10.attention.self.query_proj\n",
      "deberta.encoder.layer.10.attention.self.key_proj\n",
      "deberta.encoder.layer.10.attention.self.value_proj\n",
      "deberta.encoder.layer.10.attention.self.pos_dropout\n",
      "deberta.encoder.layer.10.attention.self.dropout\n",
      "deberta.encoder.layer.10.attention.output\n",
      "deberta.encoder.layer.10.attention.output.dense\n",
      "deberta.encoder.layer.10.attention.output.LayerNorm\n",
      "deberta.encoder.layer.10.attention.output.dropout\n",
      "deberta.encoder.layer.10.intermediate\n",
      "deberta.encoder.layer.10.intermediate.dense\n",
      "deberta.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "deberta.encoder.layer.10.output\n",
      "deberta.encoder.layer.10.output.dense\n",
      "deberta.encoder.layer.10.output.LayerNorm\n",
      "deberta.encoder.layer.10.output.dropout\n",
      "deberta.encoder.layer.11\n",
      "deberta.encoder.layer.11.attention\n",
      "deberta.encoder.layer.11.attention.self\n",
      "deberta.encoder.layer.11.attention.self.query_proj\n",
      "deberta.encoder.layer.11.attention.self.key_proj\n",
      "deberta.encoder.layer.11.attention.self.value_proj\n",
      "deberta.encoder.layer.11.attention.self.pos_dropout\n",
      "deberta.encoder.layer.11.attention.self.dropout\n",
      "deberta.encoder.layer.11.attention.output\n",
      "deberta.encoder.layer.11.attention.output.dense\n",
      "deberta.encoder.layer.11.attention.output.LayerNorm\n",
      "deberta.encoder.layer.11.attention.output.dropout\n",
      "deberta.encoder.layer.11.intermediate\n",
      "deberta.encoder.layer.11.intermediate.dense\n",
      "deberta.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "deberta.encoder.layer.11.output\n",
      "deberta.encoder.layer.11.output.dense\n",
      "deberta.encoder.layer.11.output.LayerNorm\n",
      "deberta.encoder.layer.11.output.dropout\n",
      "deberta.encoder.rel_embeddings\n",
      "deberta.encoder.LayerNorm\n",
      "pooler\n",
      "pooler.dense\n",
      "pooler.dropout\n",
      "classifier\n",
      "dropout\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
